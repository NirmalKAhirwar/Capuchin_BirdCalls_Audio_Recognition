{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeu9I3CaHPsAhh0yxJVAnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirmalKAhirwar/Capuchin_BirdCalls_Audio_Recognition/blob/master/Merging_Adapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QfaKAEu69Bs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Adapters to Base Model"
      ],
      "metadata": {
        "id": "gUmGtxS_7mxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install -U bitsandbytes\n",
        "%pip install -U transformers\n",
        "%pip install -U accelerate\n",
        "%pip install -U peft\n",
        "%pip install -U trl"
      ],
      "metadata": {
        "id": "dRIwc6du7BYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "\n",
        "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "login(token = hf_token)"
      ],
      "metadata": {
        "id": "kKrpgKsq7Bbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
        "new_model = \"/kaggle/input/fine-tune-llama-3-8b-on-medical-dataset/llama-3-8b-chat-doctor/\""
      ],
      "metadata": {
        "id": "-QULbWaF7BeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from trl import setup_chat_format\n",
        "# Reload tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        return_dict=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        ")\n",
        "\n",
        "base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n",
        "\n",
        "# Merge adapter with base model\n",
        "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
        "\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "T9I7TtFf7BhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"}]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "outputs = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "1cFXiSPI7Bjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"llama-3-8b-chat-doctor\")\n",
        "tokenizer.save_pretrained(\"llama-3-8b-chat-doctor\")"
      ],
      "metadata": {
        "id": "Lx7h18km7Spe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)\n",
        "tokenizer.push_to_hub(\"llama-3-8b-chat-doctor\", use_temp_dir=False)"
      ],
      "metadata": {
        "id": "6_uJs6bn7SsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQT_QFBN7SvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Model to Llama.cpp GGUF format"
      ],
      "metadata": {
        "id": "p8IRwL9J7fsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can’t use the safetensors files locally as most local AI chatbots don’t support them. Instead, we'll convert it into the llama.cpp GGUF file format."
      ],
      "metadata": {
        "id": "FXDCLYoc7yCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working\n",
        "!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n",
        "%cd /kaggle/working/llama.cpp\n",
        "!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n",
        "!LLAMA_CUDA=1 conda run -n base make -j > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQdWGc17Sx6",
        "outputId": "6b62bc20-0dd5-490b-9ad1-5473283fc130"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/kaggle/working'\n",
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1113, done.\u001b[K\n",
            "remote: Counting objects: 100% (1113/1113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (856/856), done.\u001b[K\n",
            "remote: Total 1113 (delta 258), reused 655 (delta 209), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1113/1113), 17.81 MiB | 16.92 MiB/s, done.\n",
            "Resolving deltas: 100% (258/258), done.\n",
            "[Errno 2] No such file or directory: '/kaggle/working/llama.cpp'\n",
            "/content\n",
            "sed: can't read Makefile: No such file or directory\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Safetensors to GGUF model format"
      ],
      "metadata": {
        "id": "7bbpN1XA78bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert-hf-to-gguf.py /kaggle/input/fine-tuned-adapter-to-full-model/llama-3-8b-chat-doctor/ \\\n",
        "    --outfile /kaggle/working/llama-3-8b-chat-doctor.gguf \\\n",
        "    --outtype f16"
      ],
      "metadata": {
        "id": "F-h8_k7P7S0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantizing the GGUF model"
      ],
      "metadata": {
        "id": "MaTJtxG18MPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular laptops don’t have enough RAM and GPU memory to load the entire model, so we have to quantify the GGUF model, reducing the 16 GB model to around 4-5 GB."
      ],
      "metadata": {
        "id": "ntiZT8q18OnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working\n",
        "!git clone --depth=1 https://github.com/ggerganov/llama.cpp.git\n",
        "%cd /kaggle/working/llama.cpp\n",
        "!sed -i 's|MK_LDFLAGS   += -lcuda|MK_LDFLAGS   += -L/usr/local/nvidia/lib64 -lcuda|' Makefile\n",
        "!LLAMA_CUDA=1 conda run -n base make -j > /dev/null"
      ],
      "metadata": {
        "id": "ayiLfOSu8IoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization\n",
        "The quantize script requires a GGUF model directory, output file directory, and quantization method. We are converting the model using the Q4_K_M method."
      ],
      "metadata": {
        "id": "iYJdVknm8gnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/\n",
        "\n",
        "!./llama.cpp/llama-quantize /kaggle/input/hf-llm-to-gguf/llama-3-8b-chat-doctor.gguf llama-3-8b-chat-doctor-Q4_K_M.gguf Q4_K_M"
      ],
      "metadata": {
        "id": "1ofQDKtJ8IrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import HfApi\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
        "login(token = hf_token)\n",
        "\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"/kaggle/working/llama-3-8b-chat-doctor-Q4_K_M.gguf\",\n",
        "    path_in_repo=\"llama-3-8b-chat-doctor-Q4_K_M.gguf\",\n",
        "    repo_id=\"kingabzpro/llama-3-8b-chat-doctor\",\n",
        "    repo_type=\"model\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "BFsbpIW58Iut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Fine-Tuned Model Locally"
      ],
      "metadata": {
        "id": "bjutWoA68syw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_cVsCky8I2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyWZTwnk8vlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kcWZ5Ok8voH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlVFdJ7v8vrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}